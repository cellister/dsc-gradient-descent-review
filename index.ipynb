{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Review\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Recall that gradient descent is a numerical approximation method for finding optimized solutions to problems with no closed form. That is, some mathematical problems are very easy to solve analytically. A trivial example is basic algebra problems which you undoubtedly saw in grade school:  \n",
    "$x + 2 = 10$ subtracting 2 from both sides you get $x = 8$. Similarly, some more complex mathematical problems such as ordinary least squares, our preliminary regression approach, also have closed-form solutions where we can follow a rote procedure and be guaranteed a solution. In other cases, this is not possible and numerical approximation methods are used to find a solution. The first instance that you witnessed of this was adding the L1 and L2 (lasso and ridge, respectively) penalties to OLS regression. In these cases, numerical approximation methods, such as gradient descent, are used in order to find optimal or near-optimal solutions.\n",
    "\n",
    "\n",
    "## Objectives \n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Describe the elements of gradient descent in the context of a logistic regression \n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Gradient descent is grounded in basic calculus theory. Whenever you have a minimum or maximum, the derivative at that point is equal to zero. This is displayed visually in the picture below; the slope of the red tangent lines is equal to the derivative of the curve at that point. As you can see, the slope of all of these horizontal tangent lines will be zero. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-descent-review/master/images/new_dxdy0.png\" alt=\"higher-order polynomial graph with horizontal bars at all minima and maxima\" width=\"400\">\n",
    "\n",
    "**The gradient is simply another term for the derivative. Typically, this is the term used when we are dealing with multivariate data. The gradient is the rate of change, which is also the slope of the line tangent.**\n",
    "\n",
    "Building upon this, gradient descent attempts to find the minimum of a function by taking successive steps in the steepest direction downhill.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-descent-review/master/images/new_gradient.png\" alt=\"on the left, a 3d plot of a higher-order polynomial. on the right, an image representing finding a specific minimum\">\n",
    "\n",
    "While this process guarantees a local minimum, the starting point and step size can affect the outcome. For example, for two different runs of gradient descent, one may lead to the global minimum while the other may lead to a local minimum.\n",
    "\n",
    "Recall that the general outline for gradient descent is:\n",
    "\n",
    "1. Define initial parameters:\n",
    "    1. Pick a starting point\n",
    "    2. Pick a step size $\\alpha$ (alpha)\n",
    "    3. Choose a maximum number of iterations; the algorithm will terminate after this many iterations if a minimum has yet to be found\n",
    "    4. (optionally) define a precision parameter; similar to the maximum number of iterations, this will terminate the algorithm early. For example, one might define a precision parameter of 0.00001, in which case if the change in the loss function were less then 0.00001, the algorithm would terminate. The idea is that we are very close to the bottom and further iterations would make a negligible difference \n",
    "2. Calculate the gradient at the current point (initially, the starting point)\n",
    "3. Take a step (of size alpha) in the direction of the gradient\n",
    "4. Repeat steps 2 and 3 until the maximum number of iterations is met, or the difference between two points is less then your precision parameter\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, you briefly reviewed that a gradient is the derivative of a function, which is the rate of change at a specific point. You then reviewed the intuition behind gradient descent, as well as some of its pitfalls. Finally, you saw a brief outline of the algorithm itself. In the next lab, you'll practice coding gradient descent and applying that to some simple mathematical functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in chemistry terms:\n",
    "    Gradient descent is a mathematical optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent. In simpler terms, it's like finding the lowest point in a hilly landscape by taking small steps downhill.\n",
    "\n",
    "Let's explain gradient descent with a chemistry example:\n",
    "\n",
    "Imagine you're at the top of a hill, and you want to reach the bottom as quickly as possible. However, you can't see the entire landscape. Instead, you can only feel the slope beneath your feet. The slope represents the gradient of the hill at your current position.\n",
    "\n",
    "In chemistry, consider a scenario where you're trying to find the minimum energy configuration of a molecule. The energy of the molecule is like the height of the hill, and you want to find the configuration that corresponds to the lowest energy state.\n",
    "\n",
    "Here's how gradient descent works in this context:\n",
    "\n",
    "Starting Point:\n",
    "You start at a random configuration of the molecule. This configuration corresponds to a certain energy level.\n",
    "\n",
    "Calculate Gradient:\n",
    "At your current configuration, you calculate the gradient of the energy function with respect to the molecular configuration. This gradient tells you the direction of the steepest ascent (positive gradient) or descent (negative gradient) of the energy.\n",
    "\n",
    "Update Configuration:\n",
    "You take a small step downhill (opposite to the gradient direction) to reach a new configuration. This step is determined by the gradient and a parameter called the learning rate, which controls the size of the step.\n",
    "\n",
    "Repeat:\n",
    "You repeat steps 2 and 3 until you reach a configuration where the energy (hill height) is minimized or until you reach a predefined stopping criterion (e.g., a maximum number of iterations).\n",
    "\n",
    "Final Configuration:\n",
    "The configuration you reach after multiple iterations represents an approximate minimum energy configuration of the molecule.\n",
    "\n",
    "In summary, gradient descent in chemistry is like slowly navigating the energy landscape of a molecule, feeling the slope (gradient) at each point, and adjusting the molecular configuration to reach the lowest energy state. It's a fundamental optimization technique used in computational chemistry to study molecular structures and properties.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
